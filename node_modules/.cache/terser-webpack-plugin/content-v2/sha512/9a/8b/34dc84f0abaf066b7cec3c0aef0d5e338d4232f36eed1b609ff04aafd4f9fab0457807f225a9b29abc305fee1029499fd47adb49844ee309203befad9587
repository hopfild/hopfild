{"code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[30],{407:function(e,t,a){\"use strict\";a.r(t);var n=a(9),i=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a(\"ContentSlotsDistributor\",{attrs:{\"slot-key\":e.$parent.slotKey}},[a(\"h1\",{attrs:{id:\"variational-autoencoders\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#variational-autoencoders\"}},[e._v(\"#\")]),e._v(\" Variational Autoencoders\")]),e._v(\" \"),a(\"h2\",{attrs:{id:\"variational-inference\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#variational-inference\"}},[e._v(\"#\")]),e._v(\" Variational Inference\")]),e._v(\" \"),a(\"h3\",{attrs:{id:\"problem\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#problem\"}},[e._v(\"#\")]),e._v(\" Problem\")]),e._v(\" \"),a(\"p\",[e._v(\"In many tasks we need to somehow find out the posterior distribution $p(z|x)$.\")]),e._v(\" \"),a(\"p\",[e._v(\"$$\\np(z|x) = \\\\frac{p(x|z)p(z)}{p(x)} = \\\\frac{p(x, z)}{p(x)}\\n$$\")]),e._v(\" \"),a(\"p\",[e._v(\"However, $p(x)$ can only be found as an integral\\n$$\\np(x) = \\\\int p(x|z)p(z) dz\\n$$\")]),e._v(\" \"),a(\"p\",[e._v(\"This integral is intractable in many cases and we need to find method to solve it. We can use Monte Carlo methods which has biased estimation of the integral, or we can use Gibson sampling which has high variance. The other technic is called \"),a(\"strong\",[e._v(\"variational inference\")]),e._v(\".\")]),e._v(\" \"),a(\"p\",[e._v(\"Let's assume that we want to find some approximation $q(z)$ of $p(z|x)$. To do so we need to minimize distance between this distributions. We can use KL-divergence here. So the task is:\\n$$\\n\\\\mathit{KL}(p||q) \\\\to \\\\text{minimize}\\n$$\\nLet's take a look at this function:\")]),e._v(\" \"),a(\"p\",[e._v(\"$$\\n\\\\begin{aligned}\\n\\\\mathit{KL}(q||p) &= -\\\\sum_{z} q(z)\\\\log{\\\\frac{p(z|x)}{q(z)}} = -\\\\sum_z q(z) \\\\log{\\\\frac{p(x, z)}{q(z)p(x)}} \\\\\\n&= -\\\\sum_z q(z)\\\\log{\\\\frac{p(x, z)}{q(z)}} + \\\\sum_z q(z) \\\\cdot \\\\log p(x) \\\\\\n&= -\\\\underbrace{\\\\sum_z q(z)\\\\log{\\\\frac{p(x, z)}{q(z)}}}_L + \\\\log p(x)\\n\\\\end{aligned}\\n$$\\nso\\n$$\\n\\\\log p(x) = \\\\mathit{KL}(q||p) + L\\n$$\")]),e._v(\" \"),a(\"p\",[e._v(\"If $x$ is given then $\\\\log p(x)$ is a constant and to minimize KL-divergence we need to maximize $L$. Term L is also known as \"),a(\"strong\",[e._v(\"lower bound\")]),e._v(\" for evdence $x$ as $L \\\\leq \\\\log p(x)$ (KL -divergence always greater or equal zero).\")]),e._v(\" \"),a(\"h3\",{attrs:{id:\"maximizing-lower-bound\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#maximizing-lower-bound\"}},[e._v(\"#\")]),e._v(\" Maximizing lower bound\")]),e._v(\" \"),a(\"p\",[e._v(\"$$\\n\\\\begin{aligned}\\nL &= \\\\sum_z q(z)\\\\log{\\\\frac{p(x, z)}{q(z)}} = \\\\sum_z q(z) \\\\log\\\\frac{p(x|z)p(z)}{q(z)} \\\\\\n&= \\\\underbrace{\\\\sum_z q(z)\\\\log p(x|z)}\"),a(\"em\",[e._v(\"{\\\\mathit{E\")]),e._v(\"{q(z)}\\\\log p(x|z)}} + \\\\underbrace{\\\\sum_z q(z) \\\\log \\\\frac{p(z)}{q(z)}}_{-\\\\mathit{KL}(q(z)|p(z))}\\n\\\\end{aligned}\\n$$\")]),e._v(\" \"),a(\"p\",[e._v(\"We can think about this in that way: to minimize $\\\\mathit{KL}(q||p)$ we need to maximize lower bound $L$; to maximize $L$ we need to minimize KL-divergence $\\\\mathit{KL}(q(z)||p(z))$ and at the same time maximize likelihood for given value $x$.\")]),e._v(\" \"),a(\"h2\",{attrs:{id:\"model\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#model\"}},[e._v(\"#\")]),e._v(\" Model\")]),e._v(\" \"),a(\"p\",[e._v(\"Now let's take a look from the outside of what we have done.\")]),e._v(\" \"),a(\"p\",[e._v(\"We take $x$ and map it to latent space $z$, and then with $p(x|z)$ trying to recinstruct $x$ with $\\\\hat{x}$.\")]),e._v(\" \"),a(\"p\",[e._v(\"We are trying to find best reconstruction $\\\\hat{x}$ for given $x$, so basically we have $p(x|\\\\hat{x})$.\")]),e._v(\" \"),a(\"p\",[e._v(\"Let's assume that $p(x|\\\\hat{x})$ has a gausian form, then\\n$$\\np(x|\\\\hat{x}) \\\\sim \\\\exp(-(x - \\\\hat{x})^2) \\\\rightarrow \\\\log(p(x|\\\\hat{x})) \\\\sim -(x - \\\\hat{x})^2.\\n$$\")]),e._v(\" \"),a(\"p\",[e._v(\"As you remember we are trying to maximize lower bound $L = \\\\mathit{E_{q(z)}\\\\log p(x|z)} + \\\\mathit{KL}(q(z)|p(z))$. So, basically when $p(x|z)$ is a gausian distribution first term is become $\\\\mathit{E_{q(z)}}-(x - \\\\hat{x})^2$ and we are trying to minimize reconstruction loss.\")]),e._v(\" \"),a(\"p\",[e._v(\"For ordinal autoencoder loss function was exactly $L_2$-loss that we have above, but for variational autoencoder we have one more requirement: we need our distribution in latent space to be the same on encoder and decoder. For example we can choose $q(z)$ to be standart $\\\\mathit{N}(0, 1)$.\")]),e._v(\" \"),a(\"h3\",{attrs:{id:\"where-is-non-determinism-in-the-model\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#where-is-non-determinism-in-the-model\"}},[e._v(\"#\")]),e._v(\" Where is non-determinism in the model?\")])])}),[],!1,null,null,null);t.default=i.exports}}]);","extractedComments":[]}